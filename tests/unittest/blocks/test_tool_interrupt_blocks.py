"""
Unit tests for tool interrupt functionality across different block types

Tests cover:
1. @tool block interrupt
2. judge block interrupt
3. explore v2 block interrupt
4. explore v1 block interrupt (both toolcall and prompt modes)
"""

import pytest
from unittest.mock import AsyncMock, Mock, patch
from typing import Dict, Any

from dolphin.core.context.context import Context
from dolphin.core.config.global_config import GlobalConfig
from dolphin.core.code_block.tool_block import ToolBlock
from dolphin.core.code_block.judge_block import JudgeBlock
from dolphin.core.code_block.explore_block import ExploreBlock
from dolphin.core.code_block.explore_block_v2 import ExploreBlockV2
from dolphin.core.utils.tools import ToolInterrupt
from dolphin.core.skill.skillkit import Skillkit
from dolphin.core.skill.skill_function import SkillFunction
from dolphin.core import flags


class MockTool:
    """Mock tool with interrupt_config"""
    
    def __init__(self, name: str, requires_confirmation: bool = True):
        self.name = name
        self.tool_name = name
        self.description = f"Test tool: {name}"
        self.inputs = {"param": {"type": "string", "description": "Test param", "required": True}}
        self.outputs = {"result": {"type": "string", "description": "Result"}}
        
        if requires_confirmation:
            self.interrupt_config = {
                "requires_confirmation": True,
                "confirmation_message": f"Confirm {name} with param={{param}}?"
            }
        else:
            self.interrupt_config = None
    
    async def arun_stream(self, **kwargs):
        """Mock tool execution"""
        yield {"result": f"Executed {self.name}"}
    
    def get_tool_json(self):
        return {
            "name": self.name,
            "description": self.description,
            "parameters": {
                "type": "object",
                "properties": {
                    "param": {"type": "string", "description": "Test param"}
                },
                "required": ["param"]
            }
        }


class TestSkillkit(Skillkit):
    """Test skillkit that provides mock tools"""
    
    def __init__(self, skills_dict: Dict[str, SkillFunction]):
        super().__init__()
        self._skills_dict = skills_dict
    
    def _createSkills(self):
        """Return list of test skills"""
        return list(self._skills_dict.values())


@pytest.fixture
def mock_context():
    """Create mock context with tools"""
    config = GlobalConfig()
    context = Context(config=config)
    
    # Convert MockTools to async functions for SkillFunction
    async def high_risk_tool(param: str = "", **kwargs):
        """High risk tool that requires confirmation
        
        Args:
            param: Test parameter for the tool
        """
        return {"result": f"High risk operation completed with param: {param}"}
    
    async def safe_tool(param: str = "", **kwargs):
        """Safe tool that doesn't require confirmation
        
        Args:
            param: Test parameter for the tool
        """
        return {"result": f"Safe operation completed with param: {param}"}
    
    # Create SkillFunction instances
    high_risk_skill = SkillFunction(high_risk_tool)
    # Add interrupt_config to the SkillFunction
    high_risk_skill.interrupt_config = {
        "requires_confirmation": True,
        "confirmation_message": "Confirm high_risk_tool with param={param}?"
    }
    
    safe_skill = SkillFunction(safe_tool)
    # No interrupt_config for safe tool
    
    # Create test skillkit with these skills
    skills_dict = {
        "high_risk_tool": high_risk_skill,
        "safe_tool": safe_skill
    }
    skillkit = TestSkillkit(skills_dict)
    
    # Use set_skills to properly initialize all_skills
    context.set_skills(skillkit)
    
    return context


class TestToolBlockInterrupt:
    """Test tool interrupt in @tool block"""
    
    @pytest.mark.asyncio
    async def test_tool_block_triggers_interrupt(self, mock_context):
        """Test that @tool block triggers interrupt for tools with interrupt_config"""
        block = ToolBlock(context=mock_context)
        
        # Tool block should raise ToolInterrupt when calling high_risk_tool
        # Use correct Dolphin syntax: @tool_name(args) -> output_var
        with pytest.raises(ToolInterrupt) as exc_info:
            async for _ in block.execute("@high_risk_tool(param='test_value') -> result"):
                pass
        
        # Verify interrupt details
        interrupt = exc_info.value
        assert interrupt.tool_name == "high_risk_tool"
        assert any(arg["key"] == "param" and arg["value"] == "test_value" 
                  for arg in interrupt.tool_args)
        # ToolInterrupt message is in args[0], not as .message attribute
        assert "Confirm high_risk_tool" in str(interrupt)
    
    @pytest.mark.asyncio
    async def test_tool_block_no_interrupt_for_safe_tool(self, mock_context):
        """Test that @tool block doesn't interrupt for tools without interrupt_config"""
        block = ToolBlock(context=mock_context)
        
        # Safe tool should execute normally without interrupt
        # Use correct Dolphin syntax: @tool_name(args) -> output_var
        results = []
        async for result in block.execute("@safe_tool(param='test_value') -> result"):
            results.append(result)
        
        # Should get results without interrupt
        assert len(results) > 0
    
    @pytest.mark.asyncio
    async def test_tool_block_resume_after_interrupt(self, mock_context):
        """Test resuming @tool block execution after interrupt"""
        block = ToolBlock(context=mock_context)
        
        # Simulate interrupted state
        mock_context.set_variable("intervention_tool_block_vars", {
            "tool_name": "high_risk_tool",
            "tool_call_info": {
                "tool_name": "high_risk_tool",
                "args": {"param": "test_value"},
                "assign_type": "->",
                "output_var": "result"
            }
        })
        
        # Simulate user confirmation with modified params
        mock_context.set_variable("tool", {
            "tool_name": "high_risk_tool",
            "tool_args": [
                {"key": "param", "value": "modified_value", "type": "str"}
            ]
        })
        
        # Execute should now run with intervention=False
        # Use correct Dolphin syntax
        results = []
        async for result in block.execute("@high_risk_tool(param='test_value') -> result"):
            results.append(result)
        
        # Should execute successfully with modified params
        assert len(results) > 0


class TestJudgeBlockInterrupt:
    """Test tool interrupt in judge block"""
    
    @pytest.mark.asyncio
    async def test_judge_block_triggers_interrupt(self, mock_context):
        """Test that judge block triggers interrupt when selecting tool with interrupt_config"""
        # Note: Judge block validation happens during parse, so we need to patch _validate_skills
        # or provide tools parameter without validation
        block = JudgeBlock(context=mock_context)
        
        # Skip validation by patching _validate_skills
        with patch.object(block, '_validate_skills'):
            # Mock LLM response to select high_risk_tool
            with patch.object(block, 'judge_tool_call', new_callable=AsyncMock) as mock_judge:
                mock_judge.return_value = ("high_risk_tool", {"param": "test_value"})
                
                # Judge block should raise ToolInterrupt
                # Use correct Dolphin syntax: /judge/(params) content -> output_var
                with pytest.raises(ToolInterrupt) as exc_info:
                    async for _ in block.execute("/judge/(tools=['high_risk_tool']) Perform high risk operation -> result"):
                        pass
                
                # Verify interrupt details
                interrupt = exc_info.value
                assert interrupt.tool_name == "high_risk_tool"
                assert any(arg["key"] == "param" for arg in interrupt.tool_args)
    
    @pytest.mark.asyncio
    async def test_judge_block_no_interrupt_for_safe_tool(self, mock_context):
        """Test that judge block doesn't interrupt when selecting safe tool"""
        block = JudgeBlock(context=mock_context)
        
        # Skip validation by patching _validate_skills
        with patch.object(block, '_validate_skills'):
            # Mock LLM response to select safe_tool
            with patch.object(block, 'judge_tool_call', new_callable=AsyncMock) as mock_judge:
                mock_judge.return_value = ("safe_tool", {"param": "test_value"})
                
                # Should execute normally without interrupt
                # Use correct Dolphin syntax
                results = []
                async for result in block.execute("/judge/(tools=['safe_tool']) Perform safe operation -> result"):
                    results.append(result)
                
                # Should get results without interrupt
                assert len(results) > 0
    
    @pytest.mark.asyncio
    async def test_judge_block_resume_after_interrupt(self, mock_context):
        """Test resuming judge block execution after interrupt"""
        block = JudgeBlock(context=mock_context)
        
        # Simulate interrupted state
        mock_context.set_variable("intervention_judge_block_vars", {
            "tool_name": "high_risk_tool",
            "judge_call_info": {
                "judge_str": "Perform operation",
                "assign_type": "->",
                "output_var": "result",
                "params": {}
            }
        })
        
        # Simulate user confirmation
        mock_context.set_variable("tool", {
            "tool_name": "high_risk_tool",
            "tool_args": [
                {"key": "param", "value": "confirmed_value", "type": "str"}
            ]
        })
        
        # Execute should now run with intervention=False
        # Use correct Dolphin syntax
        results = []
        async for result in block.execute("/judge/ Perform operation -> result"):
            results.append(result)
        
        # Should execute successfully
        assert len(results) > 0


class TestExploreBlockV2Interrupt:
    """Test tool interrupt in explore v2 block"""
    
    @pytest.mark.asyncio
    async def test_explore_v2_triggers_interrupt(self, mock_context):
        """Test that explore v2 block triggers interrupt for tools with interrupt_config"""
        # Enable V2 mode
        flags.set_flag(flags.EXPLORE_BLOCK_V2, True)
        
        try:
            block = ExploreBlockV2(context=mock_context)
            
            # Skip validation by patching _validate_skills
            with patch.object(block, '_validate_skills'):
                # Mock LLM to return tool call
                with patch.object(block, 'llm_chat_stream') as mock_llm:
                    from dolphin.core.common.enums import StreamItem
                    
                    # Create stream item with tool call
                    stream_item = StreamItem()
                    stream_item.tool_name = "high_risk_tool"
                    stream_item.tool_arguments = {"param": "test_value"}
                    stream_item.tool_call_id = "call_123"
                    
                    async def mock_stream():
                        yield stream_item
                    
                    mock_llm.return_value = mock_stream()
                    
                    # Should raise ToolInterrupt
                    # Use correct Dolphin syntax: /explore/(params) content -> output_var
                    with pytest.raises(ToolInterrupt) as exc_info:
                        async for _ in block.execute("/explore/(tools=['high_risk_tool']) Perform operation -> result"):
                            pass
                    
                    # Verify interrupt details
                    interrupt = exc_info.value
                    assert interrupt.tool_name == "high_risk_tool"
        
        finally:
            flags.reset()
    
    @pytest.mark.asyncio
    async def test_explore_v2_no_interrupt_for_safe_tool(self, mock_context):
        """Test that explore v2 doesn't interrupt for safe tools"""
        flags.set_flag(flags.EXPLORE_BLOCK_V2, True)
        
        try:
            block = ExploreBlockV2(context=mock_context)
            
            # Mock LLM to return safe tool call
            with patch.object(block, 'llm_chat_stream') as mock_llm:
                from dolphin.core.common.enums import StreamItem
                
                stream_item = StreamItem()
                stream_item.tool_name = "safe_tool"
                stream_item.tool_arguments = {"param": "test_value"}
                stream_item.tool_call_id = "call_123"
                
                async def mock_stream():
                    yield stream_item
                
                mock_llm.return_value = mock_stream()
                
                # Should execute without interrupt
                # Use correct Dolphin syntax
                # Note: This is a simplified test; actual execution is more complex
                results = []
                try:
                    async for result in block.execute("/explore/(tools=['safe_tool']) Perform operation -> result"):
                        results.append(result)
                        if len(results) > 5:  # Limit iterations
                            break
                except Exception:
                    pass  # Expected to fail due to mocking
        
        finally:
            flags.reset()


class TestExploreBlockV1Interrupt:
    """Test tool interrupt in explore v1 block (both toolcall and prompt modes)"""
    
    @pytest.mark.asyncio
    async def test_explore_v1_toolcall_mode_triggers_interrupt(self, mock_context):
        """Test that explore v1 in toolcall mode triggers interrupt"""
        # Ensure V2 is disabled
        flags.set_flag(flags.EXPLORE_BLOCK_V2, False)
        
        try:
            block = ExploreBlock(context=mock_context)
            
            # Skip validation by patching _validate_skills
            with patch.object(block, '_validate_skills'):
                # Mock llm_chat_stream (not llm_chat) - explore v1 uses llm_chat_stream
                with patch.object(block, 'llm_chat_stream') as mock_llm:
                    from dolphin.core.common.enums import StreamItem
                    
                    async def mock_stream(**kwargs):
                        # Create stream item with tool call
                        item = StreamItem()
                        item.tool_name = "high_risk_tool"
                        item.tool_args = {"param": "test_value"}  # âœ“ tool_args not tool_arguments
                        item.tool_call_id = "call_123"
                        yield item
                    
                    mock_llm.side_effect = mock_stream
                    
                    # Should raise ToolInterrupt
                    # Use correct Dolphin syntax: /explore/(params) content -> output_var
                    with pytest.raises(ToolInterrupt) as exc_info:
                        async for _ in block.execute("/explore/(tools=['high_risk_tool'], mode='tool_call') Perform operation -> result"):
                            pass
                    
                    # Verify interrupt details
                    interrupt = exc_info.value
                    assert interrupt.tool_name == "high_risk_tool"
                    
                    # Verify mode is toolcall after parsing
                    assert block.mode == "tool_call"
        
        finally:
            flags.reset()
    
    @pytest.mark.asyncio
    async def test_explore_v1_prompt_mode_triggers_interrupt(self, mock_context):
        """Test that explore v1 in prompt mode triggers interrupt"""
        flags.set_flag(flags.EXPLORE_BLOCK_V2, False)
        
        try:
            block = ExploreBlock(context=mock_context)
            
            # Skip validation by patching _validate_skills
            with patch.object(block, '_validate_skills'):
                # Mock llm_chat_stream (explore v1 uses llm_chat_stream)
                with patch.object(block, 'llm_chat_stream') as mock_llm:
                    from dolphin.core.common.enums import StreamItem
                    
                    async def mock_stream(**kwargs):
                        # Simulate LLM output with =>$ syntax in prompt mode
                        # Note: In prompt mode, tool is parsed from content, so we need
                        # In prompt mode, use =># format for tool calls
                        item = StreamItem()
                        item.answer = '=>#high_risk_tool: {"param": "test_value"}'
                        yield item
                    
                    mock_llm.side_effect = mock_stream
                    
                    # Should raise ToolInterrupt
                    # Use correct Dolphin syntax
                    with pytest.raises(ToolInterrupt) as exc_info:
                        async for _ in block.execute("/explore/(tools=['high_risk_tool'], mode='prompt') Perform operation -> result"):
                            pass
                    
                    # Verify interrupt details
                    interrupt = exc_info.value
                    assert interrupt.tool_name == "high_risk_tool"
                    
                    # Verify mode is prompt after parsing
                    assert block.mode == "prompt"
        
        finally:
            flags.reset()
    
    @pytest.mark.asyncio
    async def test_explore_v1_mode_parameter_affects_strategy(self, mock_context):
        """Test that mode parameter correctly switches between strategies"""
        flags.set_flag(flags.EXPLORE_BLOCK_V2, False)
        
        try:
            from dolphin.core.code_block.explore_strategy import ToolCallStrategy, PromptStrategy
            
            # Test toolcall mode - use correct Dolphin syntax
            block_toolcall = ExploreBlock(context=mock_context)
            # Need to execute to trigger parsing
            with patch.object(block_toolcall, 'llm_chat') as mock_llm:
                async def mock_stream():
                    yield {"content": "test"}
                mock_llm.return_value = mock_stream()
                
                try:
                    async for _ in block_toolcall.execute("/explore/(mode='tool_call') test -> result"):
                        break  # Just need to trigger parsing
                except:
                    pass  # Expected to fail due to mocking
            
            assert block_toolcall.mode == "tool_call"
            assert isinstance(block_toolcall.strategy, ToolCallStrategy)
            
            # Test prompt mode - use correct Dolphin syntax
            block_prompt = ExploreBlock(context=mock_context)
            with patch.object(block_prompt, 'llm_chat') as mock_llm:
                async def mock_stream():
                    yield {"content": "test"}
                mock_llm.return_value = mock_stream()
                
                try:
                    async for _ in block_prompt.execute("/explore/(mode='prompt') test -> result"):
                        break  # Just need to trigger parsing
                except:
                    pass  # Expected to fail due to mocking
            
            assert block_prompt.mode == "prompt"
            assert isinstance(block_prompt.strategy, PromptStrategy)
        
        finally:
            flags.reset()


class TestInterruptSkipBehavior:
    """Test skip action in tool interrupt"""
    
    @pytest.mark.asyncio
    async def test_skip_tool_continues_execution(self, mock_context):
        """Test that skipping a tool allows execution to continue"""
        block = ToolBlock(context=mock_context)
        
        # Simulate user choosing to skip
        mock_context.set_variable("intervention_tool_block_vars", {
            "tool_name": "high_risk_tool",
            "tool_call_info": {
                "tool_name": "high_risk_tool",
                "args": {"param": "test_value"},
                "assign_type": "->",
                "output_var": "result"
            }
        })
        
        # Set skip flag
        mock_context.set_variable("tool", {
            "tool_name": "high_risk_tool",
            "tool_args": [
                {"key": "param", "value": "test_value", "type": "str"}
            ]
        })
        mock_context.set_variable("__skip_tool__", True)
        mock_context.set_variable("__skip_message__", "User chose to skip this operation")
        
        # Execution should handle skip gracefully
        # Note: Actual skip handling is done at executor level
        # This test verifies block can handle skip state
        assert mock_context.get_var_value("__skip_tool__") is True


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])

